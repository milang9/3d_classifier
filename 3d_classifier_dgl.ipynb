{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a683650d-9deb-45f7-8d82-17d0203ec8f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a683650d-9deb-45f7-8d82-17d0203ec8f9",
    "outputId": "f96d99df-55d8-4280-d21d-976e8353e707"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "[13:01:05] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /home/milan/anaconda3/envs/torch/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n",
      "True\n",
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "#google colab installs:\n",
    "#!pip install dgl\n",
    "#!DGLBACKEND=pytorch\n",
    "#!export $DGLBACKEND\n",
    "#!pip install forgi\n",
    "#!tar -xf data.tar.xz\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl.dataloading as dtl\n",
    "from dgl_classifier.data import CGDataset\n",
    "\n",
    "print(th.__version__)\n",
    "print(th.cuda.is_available())\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "#Training Set\n",
    "training_dir = \"data/train_set\"\n",
    "rmsd_list = \"data/truncated_train_rmsd.txt\" #\"./data/train_rmsd_list.txt\"\n",
    "training_dataset = CGDataset(training_dir, rmsd_list, device)\n",
    "\n",
    "#Validation Set\n",
    "val_dir = \"./data/val_set\"\n",
    "val_rmsd = \"./data/val_rmsd_list.txt\"\n",
    "val_dataset = CGDataset(val_dir, val_rmsd, device)\n",
    "\n",
    "#Test Data\n",
    "test_dir = \"data/test_set\"\n",
    "test_rmsd = \"data/test_rmsd_list.txt\"\n",
    "test_dataset = CGDataset(test_dir, test_rmsd, device)\n",
    "\n",
    "#5S rRNA and tRNA Test Data (previous training data)\n",
    "st_test_dir = \"data/old_training_set\"\n",
    "st_test_rmsd = \"data/old_train_rmsd_list.txt\"\n",
    "st_test_dataset = CGDataset(st_test_dir, st_test_rmsd, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n2IxR1AxZdYn",
   "metadata": {
    "id": "n2IxR1AxZdYn"
   },
   "source": [
    "TODO:\n",
    "*  include checkpoints, that save model parameters for later testing **DONE**\n",
    "*  implement gpu usage **DONE**\n",
    "*  **ADD NAMES OF GRAPHS**\n",
    "*  implement checkpoints \n",
    "*  use dgl.save_graph() to store a graph, so the structure can be used for several steps?\n",
    "*  use forgi.threedee.model.coarse_grain.CoarseGrainRNA.rotate() to rotate cg RNAs and see if the classification changes\n",
    "*  future --> find where ernwin writes/stores output of structure for each n steps\n",
    "*  finetune the model\n",
    "*  include logger (maybe wandb?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec1a7b3-8adc-45a3-83ab-7cb02f31a4af",
   "metadata": {
    "id": "fec1a7b3-8adc-45a3-83ab-7cb02f31a4af"
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Coarse Grain RNA Classifier Model\n",
    "class CG_Classifier(th.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        self.c = 0\n",
    "        super(CG_Classifier, self).__init__()\n",
    "        \n",
    "        self.conv1 = dglnn.TAGConv(6, 100, k=2, activation=F.elu)\n",
    "        self.conv2 = dglnn.TAGConv(100, 100, k=2, activation=F.elu)\n",
    "        self.conv3 = dglnn.TAGConv(100, 100, k=2, activation=F.elu)\n",
    "        self.conv4 = dglnn.TAGConv(100, 100, k=2, activation=F.elu)\n",
    "        self.conv5 = dglnn.TAGConv(100, 25, k=2, activation=F.elu)\n",
    "        \n",
    "        self.sage_conv1 = dglnn.SAGEConv(25*num_features, 100, 'pool') # sageconv 'pool' 'gcn'\n",
    "        self.sage_conv2 = dglnn.SAGEConv(100, 100, 'pool') \n",
    "        self.sage_conv3 = dglnn.SAGEConv(100, 100, 'pool')  \n",
    "        self.sage_conv4 = dglnn.SAGEConv(100, 100, 'pool') \n",
    "        self.sage_conv5 = dglnn.SAGEConv(100, 50, 'pool')\n",
    "        \n",
    "        self.readout = dglnn.AvgPooling() #SumPooling() #dglnn.Set2Set(20, 3, 3) # AvgPooling worked best\n",
    "        \n",
    "        self.classify = th.nn.Sequential(\n",
    "            th.nn.Linear(50, 512),\n",
    "            th.nn.ELU(),\n",
    "            th.nn.Linear(512, 512),\n",
    "            th.nn.ELU(),\n",
    "            th.nn.Linear(512, 512),\n",
    "            th.nn.ELU(),\n",
    "            th.nn.Linear(512, 512),\n",
    "            th.nn.ELU(),\n",
    "            th.nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.pos = th.nn.ReLU()\n",
    "    \n",
    "    def forward(self, g):\n",
    "        #treat node features separately\n",
    "        nt = g.ndata[\"type\"]\n",
    "        nc = g.ndata[\"coord\"]\n",
    "        nw = g.ndata[\"twist\"]\n",
    "        \n",
    "        nt = self.conv1(g, nt)\n",
    "        nt = self.conv2(g, nt)\n",
    "        nt = self.conv3(g, nt)\n",
    "        nt = self.conv4(g, nt)\n",
    "        nt = self.conv5(g, nt)\n",
    "\n",
    "        nc = self.conv1(g, nc)\n",
    "        nc = self.conv2(g, nc)\n",
    "        nc = self.conv3(g, nc)\n",
    "        nc = self.conv4(g, nc)\n",
    "        nc = self.conv5(g, nc)\n",
    "        \n",
    "        nw = self.conv1(g, nw)\n",
    "        nw = self.conv2(g, nw)\n",
    "        nw = self.conv3(g, nw)\n",
    "        nw = self.conv4(g, nw)\n",
    "        nw = self.conv5(g, nw)\n",
    "        \n",
    "        #combine the features\n",
    "        g.ndata[\"combi\"] = th.cat((nt, nc, nw), 1)\n",
    "\n",
    "        combi = g.ndata[\"combi\"]\n",
    "        combi = self.sage_conv1(g, combi)\n",
    "        combi = self.sage_conv2(g, combi)\n",
    "        combi = self.sage_conv3(g, combi)\n",
    "        combi = self.sage_conv4(g, combi)\n",
    "        combi = self.sage_conv5(g, combi)\n",
    "        \n",
    "        tcw = self.readout(g, combi)\n",
    "        \n",
    "        #mean in dim 0 is used to get rid of list in list tensor.. i.e. shape [x, 1] becomes [x]\n",
    "        tcw = tcw.mean(dim=0) #dgl.mean_nodes(g, combi)\n",
    "        \n",
    "        tcw = self.classify(tcw)\n",
    "\n",
    "        return self.pos(tcw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2943be90-6647-4c26-b967-a2563a142877",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2943be90-6647-4c26-b967-a2563a142877",
    "outputId": "7fc7d5c6-6279-43e5-cdb0-e08be7ea4b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training loss 18.2617, Validation loss 23.1673, learning rate: 0.00100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000003?line=65'>66</a>\u001b[0m l \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000003?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m dgl\u001b[39m.\u001b[39munbatch(batched_graph):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000003?line=67'>68</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(graph)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000003?line=68'>69</a>\u001b[0m     l\u001b[39m.\u001b[39mappend(pred)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000003?line=70'>71</a>\u001b[0m logits \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat(l)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb Cell 3'\u001b[0m in \u001b[0;36mCG_Classifier.forward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000002?line=45'>46</a>\u001b[0m nt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(g, nt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000002?line=46'>47</a>\u001b[0m nt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(g, nt)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000002?line=47'>48</a>\u001b[0m nt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv4(g, nt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000002?line=48'>49</a>\u001b[0m nt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv5(g, nt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/milan/MS_Arbeit/3d_classifier/3d_classifier_dgl.ipynb#ch0000002?line=50'>51</a>\u001b[0m nc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(g, nc)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/dgl/nn/pytorch/conv/tagconv.py:116\u001b[0m, in \u001b[0;36mTAGConv.forward\u001b[0;34m(self, graph, feat)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/dgl/nn/pytorch/conv/tagconv.py?line=112'>113</a>\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mlocal_scope():\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/dgl/nn/pytorch/conv/tagconv.py?line=113'>114</a>\u001b[0m     \u001b[39massert\u001b[39;00m graph\u001b[39m.\u001b[39mis_homogeneous, \u001b[39m'\u001b[39m\u001b[39mGraph is not homogeneous\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/dgl/nn/pytorch/conv/tagconv.py?line=115'>116</a>\u001b[0m     norm \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mpow(graph\u001b[39m.\u001b[39;49min_degrees()\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/dgl/nn/pytorch/conv/tagconv.py?line=116'>117</a>\u001b[0m     shp \u001b[39m=\u001b[39m norm\u001b[39m.\u001b[39mshape \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m (feat\u001b[39m.\u001b[39mdim() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch/lib/python3.9/site-packages/dgl/nn/pytorch/conv/tagconv.py?line=117'>118</a>\u001b[0m     norm \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mreshape(norm, shp)\u001b[39m.\u001b[39mto(feat\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "model = CG_Classifier(\n",
    "    num_features=3 #len(graph.ndata)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "b_size = 64\n",
    "train_dataloader = dtl.pytorch.GraphDataLoader(training_dataset, batch_size=b_size, shuffle=True)#, use_ddp=True)\n",
    "val_dataloader = dtl.pytorch.GraphDataLoader(val_dataset)\n",
    "\n",
    "\n",
    "opt = th.optim.Adam(model.parameters(), lr=1e-3) #SGD(model.parameters(), lr=1e-2, momentum=0.9) #\n",
    "scheduler = th.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=100) #OneCycleLR(opt, max_lr=1e-3, steps_per_epoch=len(train_dataloader), epochs=epochs)  #\n",
    "model.train()\n",
    "\n",
    "def training_loop(model, learning_rates, train_dataloader, scheduler, opt):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    learning_rates.append(scheduler.get_last_lr()[0])\n",
    "    for iter, (batched_graph, batch_labels) in enumerate(train_dataloader):\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        l = []\n",
    "        for graph in dgl.unbatch(batched_graph):\n",
    "            pred = model(graph)\n",
    "            l.append(pred)\n",
    "    \n",
    "        logits = th.cat(l)\n",
    "        loss = F.smooth_l1_loss(logits, batch_labels, reduction='mean') # smooth_l1_loss\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    #apply lr changes according to scheme\n",
    "    scheduler.step()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    return epoch_loss\n",
    "\n",
    "@th.no_grad()\n",
    "def val_loop(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for i, (v_graph, v_label) in enumerate(val_dataloader):\n",
    "        val_pred = model(v_graph)\n",
    "        v_loss = F.smooth_l1_loss(val_pred, v_label, reduction='mean')  # smooth_l1_loss\n",
    "        val_loss += v_loss.detach().item()\n",
    "    val_loss /= (i + 1)\n",
    "    return val_loss\n",
    "\n",
    "#training setup\n",
    "val_losses = []\n",
    "epoch_losses = []\n",
    "learning_rates = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    learning_rates.append(scheduler.get_last_lr()[0])\n",
    "    for iter, (batched_graph, batch_labels) in enumerate(train_dataloader):\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        l = []\n",
    "        for graph in dgl.unbatch(batched_graph):\n",
    "            pred = model(graph)\n",
    "            l.append(pred)\n",
    "        \n",
    "        logits = th.cat(l)\n",
    "        loss = F.smooth_l1_loss(logits, batch_labels, reduction='mean') # smooth_l1_loss\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "\n",
    "    #apply lr changes according to scheme\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss /= (iter + 1)\n",
    "    #epoch_loss = training_loop(model, learning_rates, train_dataloader, scheduler, opt)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    #val setup\n",
    "    val_loss = val_loop(model, val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    #if epoch >= 20:\n",
    "    th.save(model.state_dict(), \"dgl_model_data/model_epoch\" + str(epoch) + \".pth\")\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch {}: Training loss {:.4f}, Validation loss {:.4f}, learning rate: {:.5f}\".format(epoch, epoch_loss, val_loss, scheduler.get_last_lr()[0]))\n",
    "        \n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"Training took {:.2f} hours\".format((end - start)/60/60))\n",
    "print(\"Minimum Training Loss {:.4f} in epoch {}\".format(min(epoch_losses), epoch_losses.index(min(epoch_losses))))\n",
    "print(\"Minimum Validation Loss {:.4f} in epoch {}\".format(min(val_losses), val_losses.index(min(val_losses))))\n",
    "\n",
    "#plot the training run\n",
    "plt.plot(epoch_losses)\n",
    "plt.plot(val_losses, 'r')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSD loss\")\n",
    "plt.ylim(ymax=18, ymin=0)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc0e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store training run data\n",
    "\n",
    "file = \"loss_data_2022-02-22.txt\"\n",
    "\n",
    "with open(\"data/\" + file, \"w\") as fh:\n",
    "    fh.write(str(epoch_losses) + \"\\n\")\n",
    "    fh.write(str(val_losses) + \"\\n\")\n",
    "    fh.write(str(learning_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training run data\n",
    "\n",
    "file = \"loss_data_2022-02-22.txt\"\n",
    "\n",
    "file_lines = []\n",
    "with open(\"data/\" + file, \"r\") as fh:\n",
    "    for line in fh.readlines():\n",
    "        file_lines.append(line.rstrip(\"]\\n\").lstrip(\"[\").split(\",\"))\n",
    "\n",
    "epoch_losses = [float(a) for a in file_lines[0]]\n",
    "val_losses = [float(b) for b in file_lines[1]]\n",
    "learning_rates = [float(c) for c in file_lines[2]]\n",
    "\n",
    "print(\"Minimum Training Loss {:.4f} in epoch {}\".format(min(epoch_losses), epoch_losses.index(min(epoch_losses))))\n",
    "print(\"Minimum Validation Loss {:.4f} in epoch {}\".format(min(val_losses), val_losses.index(min(val_losses))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b41a5-5213-49df-bd67-9984f67183f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "ed5b41a5-5213-49df-bd67-9984f67183f5",
    "outputId": "4a0594fa-255d-4bf4-fb4b-55f761a3b36c"
   },
   "outputs": [],
   "source": [
    "#plot the training run\n",
    "fig, ax1 = plt.subplots(layout='constrained', figsize=(18, 6))\n",
    "ax1.secondary_yaxis('left')\n",
    "ax1.plot(epoch_losses)\n",
    "ax1.plot(val_losses, 'r')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.secondary_yaxis('right')\n",
    "ax2.plot(learning_rates, 'g')\n",
    "plt.title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"RMSD loss\")\n",
    "ax2.set_ylabel(\"Learning rate\")\n",
    "ax1.set_ybound(lower=0, upper=25)\n",
    "plt.axvline(x = 430, c = 'black', ls = ':')\n",
    "#plt.axvline(x = 96, c = 'black', ls = ':')\n",
    "#plt.axvline(x = 145, c = 'black', ls = ':')\n",
    "#plt.axvline(x = 190, c = 'black', ls = ':')\n",
    "#plt.axvline(x = 250, c = 'black', ls = ':')\n",
    "#plt.axvline(x = 321, c = 'black', ls = ':')\n",
    "#plt.axvline(x = 404, c = 'black', ls = ':')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Loop\n",
    "import math\n",
    "from dgl_classifier.utility import loss_plot, rmsd_scatter, e_rmsd_scatter\n",
    "@th.no_grad()\n",
    "def test_loop(model, loader, e_dict, title):\n",
    "    model.eval()\n",
    "    max_label = 0\n",
    "    max_loss = 0\n",
    "    max_pred = 0\n",
    "    min_label = math.inf\n",
    "    min_loss = math.inf\n",
    "    min_pred = math.inf\n",
    "    test_losses = []\n",
    "    true_rmsds = []\n",
    "    pred_rmsds = []\n",
    "    energies = []\n",
    "    trmsds_f_en = []\n",
    "    prmsds_f_en = []\n",
    "    for test_graph, test_label in loader:\n",
    "        test_graph = test_graph.to(device)\n",
    "        test_pred = model(test_graph)\n",
    "        test_loss = F.l1_loss(test_pred, test_label).item() #smooth_l1_loss\n",
    "        test_losses.append(float(test_loss))\n",
    "        true_rmsds.append(float(test_label))\n",
    "        pred_rmsds.append(float(test_pred))\n",
    "\n",
    "        #if test_graph.name[0] in e_dict.keys():\n",
    "        #    energies.append(e_dict[test_graph.name[0]])\n",
    "        #    prmsds_f_en.append(float(test_pred))\n",
    "        #    trmsds_f_en.append(float(test_graph.y))\n",
    "        if test_loss > max_loss:\n",
    "            max_loss = test_loss\n",
    "            max_label = test_label\n",
    "            max_pred = test_pred\n",
    "        if test_loss < min_loss:\n",
    "            min_loss = test_loss\n",
    "            min_label = test_label\n",
    "            min_pred = test_pred\n",
    "        if test_pred < 0:\n",
    "            print(test_label, test_pred)\n",
    "    \n",
    "    print(title)\n",
    "    print(min_label, min_pred, min_loss)\n",
    "    print(max_label, max_pred, max_loss)\n",
    "    test_mean = np.mean(test_losses)\n",
    "    test_std = np.std(test_losses)\n",
    "    test_fq = np.quantile(test_losses, q = 0.25)\n",
    "    test_median = np.median(test_losses)\n",
    "    test_tq = np.quantile(test_losses, q = 0.75)\n",
    "    print(\"Mean Test loss: \\t {:.4f}\".format(test_mean))\n",
    "    print(\"Std. Dev. of Test loss:  {:.4f}\".format(test_std))\n",
    "    print(\"Min loss: \\t\\t {:.4f}\".format(min(test_losses)))\n",
    "    print(\"First Quantile: \\t {:.4f}\".format(test_fq))\n",
    "    print(\"Median: \\t\\t {:.4f}\".format(test_median))\n",
    "    print(\"Third Quantile: \\t {:.4f}\".format(test_tq))\n",
    "    print(\"Max Loss: \\t\\t {:.4f}\".format(max(test_losses)))\n",
    "    \n",
    "    loss_plot(test_losses, test_fq, test_median, test_tq, title + \", Sorted Test Losses\")\n",
    "    rmsd_scatter(pred_rmsds, true_rmsds, title)\n",
    "    e_rmsd_scatter(energies, trmsds_f_en, title + \", True RMSDs vs Energy\")\n",
    "    e_rmsd_scatter(energies, prmsds_f_en, title + \", Predicted RMSDs vs Energy\")\n",
    "    return #energies, trmsds_f_en, prmsds_f_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eUo0_OJpxrV4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "eUo0_OJpxrV4",
    "outputId": "6bde702c-94a8-45dc-a654-8776f0b9a111"
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "import math\n",
    "from dgl_classifier.utility import get_energy_dict\n",
    "\n",
    "if \"model\" not in globals():\n",
    "    model = CG_Classifier(num_features=3)\n",
    "\n",
    "train_loader = dtl.pytorch.GraphDataLoader(training_dataset)\n",
    "val_loader = dtl.pytorch.GraphDataLoader(val_dataset)\n",
    "test_dataloader = dtl.pytorch.GraphDataLoader(test_dataset)\n",
    "st_test_dataloader = dtl.pytorch.GraphDataLoader(st_test_dataset)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(th.load(\"dgl_model_data/model_epoch7.pth\"))\n",
    "\n",
    "\n",
    "energy_l = []\n",
    "true_rmsd_l = []\n",
    "pred_rmsd_l = []\n",
    "for loader in [test_dataloader, st_test_dataloader, train_loader, val_loader]:\n",
    "    if loader == test_dataloader:\n",
    "        title = \"Test Set\"\n",
    "        e_dict = get_energy_dict(\"data/test_energy.txt\")\n",
    "    elif loader == st_test_dataloader:\n",
    "        title = \"5S and tRNA Test Set\"\n",
    "        e_dict = get_energy_dict(\"data/old_train_energy.txt\")\n",
    "    elif loader == train_loader:\n",
    "        title = \"Training Set\"\n",
    "        e_dict = get_energy_dict(\"data/train_energy.txt\")\n",
    "    elif loader == val_loader:\n",
    "        title = \"Validation Set\"\n",
    "        e_dict = get_energy_dict(\"data/val_energy.txt\")\n",
    "    test_loop(model, loader, e_dict, title)\n",
    "    #en, trs, prs = test_loop(model, loader, e_dict, title)\n",
    "    #energy_l += en\n",
    "    #true_rmsd_l += trs\n",
    "    #pred_rmsd_l += prs\n",
    "\n",
    "#e_rmsd_scatter(energy_l, true_rmsd_l, \"All structures, True RMSDS vs Energies\")\n",
    "#e_rmsd_scatter(energy_l, pred_rmsd_l, \"All structures, Predicted RMSDS vs Energies\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3d_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a683650d-9deb-45f7-8d82-17d0203ec8f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a683650d-9deb-45f7-8d82-17d0203ec8f9",
    "outputId": "8448f4fe-9bbc-434a-baab-24ce6537aa2f"
   },
   "outputs": [],
   "source": [
    "#!pip install dgl\n",
    "#!DGLBACKEND=pytorch\n",
    "#!export $DGLBACKEND\n",
    "#import os\n",
    "#os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "#print(os.environ[\"DGLBACKEND\"])\n",
    "import dgl\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import torch as th\n",
    "\n",
    "#!pip install forgi\n",
    "import forgi\n",
    "import forgi.graph.bulge_graph as fgb\n",
    "import forgi.threedee as ft\n",
    "import forgi.threedee.model.coarse_grain as ftmc\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "_MuRd2MIU0bz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MuRd2MIU0bz",
    "outputId": "d66ab529-6cd4-42ba-c469-b020e7a01fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(th.__version__)\n",
    "print(th.cuda.is_available())\n",
    "#!xz -d -v data.tar.xz\n",
    "#!tar -xf data.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n2IxR1AxZdYn",
   "metadata": {
    "id": "n2IxR1AxZdYn"
   },
   "source": [
    "\n",
    "Ideas: \n",
    "*   use dgl.save_graph() to store a graph, so the structure can be used for several steps?\n",
    "*   use forgi.threedee.model.coarse_grain.CoarseGrainRNA.rotate() to rotate cg RNAs and see if the classification changes\n",
    "\n",
    "TODO:\n",
    "*  future --> find where ernwin writes/stores output of structure for each n steps\n",
    "*  finetune the model\n",
    "*  make larger batch of training data for testing\n",
    "*  include logger (maybe wandb?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81afa4b6-ee92-4662-826b-de338ca595cf",
   "metadata": {
    "id": "81afa4b6-ee92-4662-826b-de338ca595cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Graph Building\n",
    "\n",
    "#load coarse grain file\n",
    "def load_cg_file(file): \n",
    "    cg = ftmc.CoarseGrainRNA.from_bg_file(file) \n",
    "    c_dict = dict(cg.coords)\n",
    "    t_dict = dict(cg.twists)\n",
    "    coord_dict = {}\n",
    "    twist_dict = {}\n",
    "    for e in c_dict:\n",
    "        a = th.from_numpy(c_dict[e][0])\n",
    "        b = th.from_numpy(c_dict[e][1])\n",
    "        coord_dict[e] = a, b\n",
    "        if e in t_dict:\n",
    "            c = th.from_numpy(t_dict[e][0])\n",
    "            d = th.from_numpy(t_dict[e][1])\n",
    "            twist_dict[e] = c, d\n",
    "        \n",
    "    # Get elements and neighbours:\n",
    "    connections = {}\n",
    "    for elem in cg.sorted_element_iterator():\n",
    "        neighbours = cg.connections(elem)\n",
    "        if elem not in connections:\n",
    "            connections[elem] = cg.connections(elem)\n",
    "    return coord_dict, twist_dict, connections\n",
    "\n",
    "def build_dgl_graph(coord_dict, twist_dict, connections):\n",
    "    #dictionary to convert type\n",
    "    type_transl = {\n",
    "        \"h\": [1, 0, 0, 0, 0, 0],\n",
    "        \"i\": [0, 1, 0, 0, 0, 0],\n",
    "        \"m\": [0, 0, 1, 0, 0, 0],\n",
    "        \"s\": [0, 0, 0, 1, 0, 0],\n",
    "        \"f\": [0, 0, 0, 0, 1, 0],\n",
    "        \"t\": [0, 0, 0, 0, 0, 1]\n",
    "    } \n",
    "\n",
    "    #encode nodes numerically for dgl graph\n",
    "    num_graph = {}\n",
    "    elem_count = {}\n",
    "    for num, n in enumerate(sorted(connections)):\n",
    "        num_graph[n] = num\n",
    "        if n[0] not in elem_count:\n",
    "            elem_count[n[0]] = 1\n",
    "        else:\n",
    "            elem_count[n[0]] += 1\n",
    "\n",
    "    #build graph and edges\n",
    "    u = []\n",
    "    v = []\n",
    "    for node in connections:\n",
    "        for c in connections[node]:\n",
    "            u.append(num_graph[node])\n",
    "            v.append(num_graph[c])\n",
    "\n",
    "    graph = dgl.graph((th.tensor(u), th.tensor(v)))\n",
    "\n",
    "    #initialise node attributes\n",
    "    graph.ndata[\"type\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32)\n",
    "    graph.ndata[\"coord\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32) #seperate coords into 2 sets of 3, so that the information of start and end is added?\n",
    "    graph.ndata[\"twist\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32)\n",
    "\n",
    "    for elem in connections:\n",
    "        graph.ndata[\"type\"][num_graph[elem]] = th.tensor(type_transl[elem[0]], dtype=th.float32) \n",
    "        graph.ndata[\"coord\"][num_graph[elem]] = th.tensor(np.concatenate(coord_dict[elem]), dtype=th.float32)\n",
    "        if elem in twist_dict:\n",
    "            graph.ndata[\"twist\"][num_graph[elem]] = th.tensor(np.concatenate(twist_dict[elem]), dtype=th.float32)\n",
    "  \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65010c86-88ed-4940-b36f-5af93ea50a6d",
   "metadata": {
    "id": "65010c86-88ed-4940-b36f-5af93ea50a6d"
   },
   "outputs": [],
   "source": [
    "#create a dict with name and rmsd as labels\n",
    "def get_rmsd_dict(rmsd_list):\n",
    "    #rmsd_list = \"./play_set/RMSD_list.txt\"\n",
    "    rmsd_dict = {}\n",
    "    with open(rmsd_list, \"r\") as fh:\n",
    "        for line in fh.readlines():\n",
    "            name, rmsd = (line.rstrip()).split(\"\\t\")\n",
    "            rmsd_dict[name] = float(rmsd)\n",
    "    return rmsd_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ZU_znRFH141n",
   "metadata": {
    "id": "ZU_znRFH141n"
   },
   "outputs": [],
   "source": [
    "#Graph Dataset Class\n",
    "#TODO: adapt, so it can stand alone\n",
    "\n",
    "from dgl.data import DGLDataset\n",
    "class CGDataset(DGLDataset):\n",
    "    def __init__(self, directory, rmsd_list):\n",
    "        self.file_path = directory\n",
    "        self.rmsd_list = rmsd_list\n",
    "        super(CGDataset, self).__init__(name=\"cgRNA\")\n",
    "        \n",
    "        \n",
    "    def process(self):\n",
    "        self.graphs = []\n",
    "        rmsd_dict = get_rmsd_dict(self.rmsd_list)\n",
    "        self.labels = []\n",
    "        \n",
    "        files = []\n",
    "        filenames = next(os.walk(self.file_path), (None, None, []))[2]\n",
    "\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".cg\"):\n",
    "                files.append(file)\n",
    "                self.labels.append(rmsd_dict[file])\n",
    "\n",
    "        for struc in files:\n",
    "            coord_dict, twist_dict, connections = load_cg_file(os.path.join(self.file_path, struc))\n",
    "            graph = build_dgl_graph(coord_dict, twist_dict, connections)\n",
    "            self.graphs.append(build_dgl_graph(coord_dict, twist_dict, connections))\n",
    "\n",
    "        self.labels = th.tensor(self.labels)\n",
    "  \n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    #def save(self):\n",
    "     #   dgl.save_graphs(\"./play_set/training_cg_graphs.dgl\", self.graphs, labels=self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "j4O333bvAQ1V",
   "metadata": {
    "id": "j4O333bvAQ1V"
   },
   "outputs": [],
   "source": [
    "#Dataloading\n",
    "import dgl.dataloading as dtl\n",
    "\n",
    "b_size = 50\n",
    "\n",
    "#load from cg files directly\n",
    "\n",
    "training_dir = \"./data/training_set\"\n",
    "rmsd_list = \"./data/train_rmsd_list.txt\"\n",
    "\n",
    "training_dataset = CGDataset(training_dir, rmsd_list)\n",
    "\n",
    "#add randomisation as in Defining Data Loader from https://docs.dgl.ai/tutorials/blitz/5_graph_classification.html\n",
    "train_dataloader = dtl.pytorch.GraphDataLoader(training_dataset, batch_size=b_size, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bq8dpernrILf",
   "metadata": {
    "id": "bq8dpernrILf"
   },
   "outputs": [],
   "source": [
    "#Validation set\n",
    "val_dir = \"./data/val_set\"\n",
    "val_rmsd = \"./data/val_rmsd_list.txt\"\n",
    "\n",
    "val_dataset = CGDataset(val_dir, val_rmsd)\n",
    "\n",
    "val_dataloader = dtl.pytorch.GraphDataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebadfa-2a66-46ea-ac4d-e6ff32b2205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perhaps a module to coarsen/pool graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fec1a7b3-8adc-45a3-83ab-7cb02f31a4af",
   "metadata": {
    "id": "fec1a7b3-8adc-45a3-83ab-7cb02f31a4af"
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "import dgl.nn as dglnn\n",
    "from dgl.nn import GraphConv\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.geometry import neighbor_matching\n",
    "\n",
    "# feed the 3 different node attributes one after the other though the first layer? like in https://discuss.dgl.ai/t/getting-started-with-multiple-node-features-in-homogenous-graph/919/2\n",
    "# condense the 3 node attributes down to 1? see point above\n",
    "\n",
    "\n",
    "#Coarse Grain RNA Classifier Model\n",
    "class CG_Classifier(th.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        self.c = 0\n",
    "        super(CG_Classifier, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        self.conv1 = GraphConv(6, 48, activation=F.relu)\n",
    "        self.conv2 = GraphConv(48, 24, activation=F.relu)\n",
    "        self.conv3 = GraphConv(24, 12, activation=F.relu)\n",
    "        '''\n",
    "        self.conv1 = dglnn.TAGConv(6, 64, k=2, activation=F.relu)\n",
    "        self.conv2 = dglnn.TAGConv(64, 32, k=2, activation=F.relu)\n",
    "        self.conv3 = dglnn.TAGConv(32, 24, k=1, activation=F.relu)\n",
    "        \n",
    "        self.max_pool = dgl.nn.MaxPooling()\n",
    "        \n",
    "        self.sage_conv1 = dglnn.SAGEConv(24*num_features, 20, 'pool') \n",
    "        self.sage_conv2 = dglnn.SAGEConv(20, 16, 'pool')\n",
    "\n",
    "        self.dense1 = th.nn.Linear(16, 512)\n",
    "        self.dense2 = th.nn.Linear(512, 512)\n",
    "        self.dense3 = th.nn.Linear(512, 512)\n",
    "        self.classify = th.nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        nt = g.ndata[\"type\"]\n",
    "        nc = g.ndata[\"coord\"]\n",
    "        nw = g.ndata[\"twist\"]\n",
    "        \n",
    "        nt = self.conv1(g, nt)\n",
    "        #nt = self.max_pool(g, nt)\n",
    "        nt = self.conv2(g, nt)\n",
    "        #nt = self.max_pool(g, nt)\n",
    "        nt = self.conv3(g, nt)\n",
    "        #nt = self.max_pool(g, nt)\n",
    "\n",
    "        nc = self.conv1(g, nc)\n",
    "        #nc = self.max_pool(g, nc)\n",
    "        nc = self.conv2(g, nc)\n",
    "        #nc = self.max_pool(g, nc)\n",
    "        nc = self.conv3(g, nc)\n",
    "        #nc = self.max_pool(g, nc)\n",
    "        \n",
    "        nw = self.conv1(g, nw)\n",
    "        #nw = self.max_pool(g, nw)\n",
    "        nw = self.conv2(g, nw)\n",
    "        #nw = self.max_pool(g, nw)\n",
    "        nw = self.conv3(g, nw)\n",
    "        #nw = self.max_pool(g, nw)\n",
    "\n",
    "        #TODO: Modify the readout function (maybe with the local scope below)\n",
    "        \n",
    "        #use pooling to have still a graph representation, after 2 layers of seperate conv\n",
    "        #--> let conv run over the pooled graph\n",
    "        #with g.local_scope():\n",
    "        #    print(th.cat((nt, nc, nw), 0))\n",
    "        #    g.ndata[\"combi\"] = th.cat((nt, nc, nw), 0)\n",
    "        #    tcw = g.ndata[\"combi\"]\n",
    "        #    tcw = self.conv3(g, tcw)\n",
    "        #    tcw = self.max_pool(g, tcw)\n",
    "            #tcw = th.cat((nt, nc, nw), 1) #use this for graph gen and again graph con\n",
    "\n",
    "\n",
    "        #TODO: add readout function\n",
    "        #      how to best use pooling?\n",
    "        #tcw = th.cat((nt, nc, nw), 1)\n",
    "        #if self.c == 0:\n",
    "         #   print(tcw)\n",
    "        g.ndata[\"combi\"] = th.cat((nt, nc, nw), 1) #tcw\n",
    "        if self.c == 0:\n",
    "            print(g.ndata[\"combi\"])\n",
    "            print(g)\n",
    "            new_g = neighbor_matching(g)\n",
    "            print(new_g)\n",
    "        combi = g.ndata[\"combi\"]\n",
    "        combi = self.sage_conv1(g, combi)\n",
    "        \n",
    "        #g.update_all(fn.copy_u(\"combi\", \"pool1\"), fn.max()) #find out what could be target and destination for fn.copy_u\n",
    "        #combi = g.ndata[\"pool1\"]\n",
    "        \n",
    "        combi = self.sage_conv2(g, combi)\n",
    "        combi = self.max_pool(g, combi)\n",
    "\n",
    "        tcw_mean = combi #dgl.mean_nodes(g, combi)\n",
    "        if self.c == 0:\n",
    "            print(tcw_mean)\n",
    "            self.c = 1\n",
    "        #is the mean the right approach?\n",
    "        #tcw_mean = tcw.mean(dim=0)\n",
    "        tcw_mean = self.dense1(tcw_mean)\n",
    "        tcw_mean = self.dense2(tcw_mean)\n",
    "        tcw_mean = self.dense3(tcw_mean)\n",
    "        \n",
    "        return self.classify(tcw_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2943be90-6647-4c26-b967-a2563a142877",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "2943be90-6647-4c26-b967-a2563a142877",
    "outputId": "a9353659-684a-43c7-9a04-8c960be46684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0629e+00, 3.7466e-01, 1.5988e-01, 0.0000e+00, 3.1491e-01, 5.8409e-04,\n",
      "         2.2554e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3051e-02,\n",
      "         0.0000e+00, 3.0364e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1227e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3677e-01, 0.0000e+00,\n",
      "         0.0000e+00, 3.6737e+01, 1.3227e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5490e+01, 2.3120e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 9.3639e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3950e+01,\n",
      "         2.1350e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5979e+01, 0.0000e+00,\n",
      "         4.3804e-01, 4.2663e-01, 1.5262e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 5.3322e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3870e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8489e-02,\n",
      "         2.1767e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0337e+00, 3.5162e-01, 1.5189e-01, 0.0000e+00, 2.8809e-01, 0.0000e+00,\n",
      "         2.2805e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1268e-01,\n",
      "         0.0000e+00, 3.8605e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6593e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2500e-01, 0.0000e+00,\n",
      "         8.1481e+00, 2.7096e+01, 2.0307e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.7084e+00, 2.4398e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7582e+00,\n",
      "         0.0000e+00, 8.8503e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9332e+00,\n",
      "         8.1801e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5415e+00, 0.0000e+00,\n",
      "         3.4125e-01, 5.8918e-01, 4.9270e-02, 0.0000e+00, 2.3107e-02, 7.1085e-03,\n",
      "         6.0954e-01, 1.1086e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.6800e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8041e-01,\n",
      "         1.9810e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0629e+00, 3.7466e-01, 1.5988e-01, 0.0000e+00, 3.1491e-01, 5.8410e-04,\n",
      "         2.2554e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3052e-02,\n",
      "         0.0000e+00, 3.0364e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1227e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3677e-01, 0.0000e+00,\n",
      "         0.0000e+00, 2.6172e+01, 6.2874e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1672e+01, 9.8131e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 6.9221e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9077e+01,\n",
      "         2.3002e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6589e+00, 0.0000e+00,\n",
      "         3.3580e-01, 2.6186e-01, 4.0928e-02, 0.0000e+00, 2.1502e-02, 1.3874e-01,\n",
      "         6.9402e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.1793e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9489e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7915e-02, 1.0296e-01, 0.0000e+00],\n",
      "        [1.0572e+00, 4.9254e-01, 3.8171e-01, 0.0000e+00, 3.4699e-01, 3.2117e-01,\n",
      "         3.8560e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 5.9928e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2144e-01,\n",
      "         4.9490e-01, 2.3333e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.0119e+01, 7.2189e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8009e+01, 1.8041e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.3194e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9204e+01,\n",
      "         5.3323e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2842e+01, 0.0000e+00,\n",
      "         5.0842e-01, 5.4466e-01, 2.0168e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.8817e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3423e-01,\n",
      "         4.4548e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [8.6903e-01, 4.6254e-01, 2.0865e-01, 0.0000e+00, 2.5808e-01, 2.1706e-01,\n",
      "         3.4861e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8331e-01,\n",
      "         4.0290e-01, 1.7231e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.9981e+01, 1.9245e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9889e+01, 2.8401e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0554e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2977e+01,\n",
      "         1.4746e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8981e+01, 0.0000e+00,\n",
      "         5.8095e-01, 6.7778e-01, 1.1846e-01, 0.0000e+00, 0.0000e+00, 5.3494e-03,\n",
      "         3.3319e-01, 1.5191e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.0714e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8993e-01,\n",
      "         3.6844e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [8.6903e-01, 4.6254e-01, 2.0865e-01, 0.0000e+00, 2.5808e-01, 2.1706e-01,\n",
      "         3.4861e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8331e-01,\n",
      "         4.0290e-01, 1.7231e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.0320e+00, 2.7500e+01, 1.0970e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5352e+01, 1.4948e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7485e+01,\n",
      "         1.1314e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2188e+01, 0.0000e+00,\n",
      "         5.7443e-01, 5.1281e-01, 6.8629e-02, 0.0000e+00, 4.1498e-02, 9.3756e-02,\n",
      "         8.8644e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.1671e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5813e-01,\n",
      "         3.7937e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0572e+00, 4.9254e-01, 3.8171e-01, 0.0000e+00, 3.4699e-01, 3.2117e-01,\n",
      "         3.8560e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 5.9928e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2144e-01,\n",
      "         4.9490e-01, 2.3333e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.2739e+01, 3.3997e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5990e+01, 8.9664e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.7345e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5585e+01,\n",
      "         9.3443e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9226e+00, 0.0000e+00,\n",
      "         5.6324e-01, 4.1823e-01, 9.6972e-02, 0.0000e+00, 6.8977e-02, 0.0000e+00,\n",
      "         5.2344e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.3577e-01,\n",
      "         3.2026e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5651e-01, 0.0000e+00],\n",
      "        [3.9716e-01, 5.2649e-01, 4.2830e-01, 0.0000e+00, 4.2172e-02, 6.9750e-01,\n",
      "         6.2183e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8112e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7200e-03, 0.0000e+00,\n",
      "         6.0935e-01, 4.4978e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.8355e+00, 1.5281e+01, 6.7965e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.7037e+00, 1.5335e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0557e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9122e+01,\n",
      "         6.1889e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9017e+00, 0.0000e+00,\n",
      "         2.7151e-01, 8.3048e-01, 2.3982e-01, 0.0000e+00, 0.0000e+00, 1.1251e-01,\n",
      "         6.0564e-01, 2.2260e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0735e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.4520e-01, 0.0000e+00, 0.0000e+00, 3.0682e-01, 0.0000e+00],\n",
      "        [3.9752e-01, 6.9027e-01, 3.9852e-01, 0.0000e+00, 0.0000e+00, 6.4542e-01,\n",
      "         5.4960e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3902e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.4726e-01, 3.2132e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.2013e+01, 1.6974e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.2799e+01, 4.1845e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.1989e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6342e+01,\n",
      "         2.4625e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6400e+00, 0.0000e+00,\n",
      "         5.4521e-01, 7.2073e-01, 4.4704e-01, 0.0000e+00, 0.0000e+00, 3.0719e-01,\n",
      "         4.8026e-01, 1.8145e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7452e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.3617e-01, 0.0000e+00, 0.0000e+00, 6.5298e-02, 0.0000e+00],\n",
      "        [4.2413e-01, 6.9776e-01, 3.8388e-01, 0.0000e+00, 0.0000e+00, 6.3129e-01,\n",
      "         4.8793e-01, 2.4615e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0461e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.9490e-01, 3.4465e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3046e+01, 4.0911e+01, 2.4528e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.4800e+01, 3.1282e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 6.4610e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6655e+01,\n",
      "         1.5937e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6729e+01, 0.0000e+00,\n",
      "         3.6620e-01, 3.8177e-01, 3.1841e-01, 0.0000e+00, 0.0000e+00, 3.0498e-01,\n",
      "         5.1017e-01, 4.9642e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 3.2825e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3286e-01,\n",
      "         1.2444e-01, 8.5033e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.9752e-01, 6.9027e-01, 3.9852e-01, 0.0000e+00, 0.0000e+00, 6.4542e-01,\n",
      "         5.4960e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3902e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.4726e-01, 3.2132e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 2.7821e+01, 6.4409e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.9183e+00, 1.7718e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.3162e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2963e+01,\n",
      "         5.6176e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7161e+00, 0.0000e+00,\n",
      "         3.3265e-01, 7.6332e-01, 2.0425e-01, 0.0000e+00, 0.0000e+00, 2.5950e-01,\n",
      "         8.6835e-01, 5.1697e-01, 0.0000e+00, 0.0000e+00, 4.6993e-02, 3.0431e-01,\n",
      "         0.0000e+00, 9.1102e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0016e-01, 4.5023e-01, 0.0000e+00, 3.6134e-03, 1.1462e-01, 0.0000e+00]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Graph(num_nodes=11, num_edges=22,\n",
      "      ndata_schemes={'type': Scheme(shape=(6,), dtype=torch.float32), 'coord': Scheme(shape=(6,), dtype=torch.float32), 'twist': Scheme(shape=(6,), dtype=torch.float32), 'combi': Scheme(shape=(72,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 3, 4, 1, 2])\n",
      "tensor([[ 23.5117,  85.3229,  71.8796,  86.4442, -32.9648,  56.8107, -25.9912,\n",
      "          43.4891, -27.9861,  39.6969,  40.5243, -33.9195,  52.8949,  12.4687,\n",
      "         -13.9595,  -9.9795]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2495/2029319101.py:29: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.smooth_l1_loss(logits, batch_labels, reduction='mean')\n",
      "/tmp/ipykernel_2495/2029319101.py:43: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  v_loss = F.smooth_l1_loss(val_pred, v_label, reduction='mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training loss 45.8374, Validation loss 10.8239\n",
      "Epoch 5: Training loss 7.6567, Validation loss 6.8255\n",
      "Epoch 10: Training loss 7.1585, Validation loss 6.4541\n",
      "Epoch 15: Training loss 7.1866, Validation loss 6.9040\n",
      "Epoch 20: Training loss 6.8898, Validation loss 6.5529\n",
      "Epoch 25: Training loss 6.8954, Validation loss 6.9836\n",
      "Epoch 30: Training loss 6.8669, Validation loss 6.6039\n",
      "Epoch 35: Training loss 6.8556, Validation loss 6.4902\n",
      "Epoch 40: Training loss 7.2569, Validation loss 6.8886\n",
      "Epoch 45: Training loss 6.9250, Validation loss 6.8260\n",
      "Epoch 50: Training loss 7.1638, Validation loss 6.5393\n",
      "Epoch 55: Training loss 6.7703, Validation loss 6.4281\n",
      "Epoch 60: Training loss 6.9056, Validation loss 6.4109\n",
      "Epoch 65: Training loss 7.0209, Validation loss 6.4379\n",
      "Epoch 70: Training loss 6.7839, Validation loss 6.4851\n",
      "Epoch 75: Training loss 6.8640, Validation loss 6.7926\n",
      "Epoch 80: Training loss 6.8588, Validation loss 6.6865\n",
      "Epoch 85: Training loss 6.8499, Validation loss 6.5404\n",
      "Epoch 90: Training loss 6.9599, Validation loss 6.6243\n",
      "Epoch 95: Training loss 6.8925, Validation loss 6.9214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.draw()>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training\n",
    "#TODO:  build a deeper neural network and refine pooling\n",
    "#       tinker with hyperparameters\n",
    "\n",
    "model = CG_Classifier(\n",
    "    num_features=3 #len(graph.ndata)\n",
    ")\n",
    "\n",
    "opt = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "#val setup\n",
    "val_losses = []\n",
    "\n",
    "#training setup\n",
    "epoch_losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for iter, (batched_graph, batch_labels) in enumerate(train_dataloader):\n",
    "        l = []\n",
    "        for graph in dgl.unbatch(batched_graph): #ist there a way to use directly the batched_graph object?\n",
    "            pred = model(graph)\n",
    "            l.append(pred)\n",
    "        \n",
    "        logits = th.cat(l)\n",
    "        loss = F.smooth_l1_loss(logits, batch_labels, reduction='mean')\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    \n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    #val setup\n",
    "    val_loss = 0\n",
    "    for i, (v_graph, v_label) in enumerate(val_dataloader):\n",
    "        val_pred = model(v_graph)\n",
    "        v_loss = F.smooth_l1_loss(val_pred, v_label, reduction='mean')\n",
    "        val_loss += v_loss.detach().item()\n",
    "\n",
    "    val_loss /= (i + 1)\n",
    "    \n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch {}: Training loss {:.4f}, Validation loss {:.4f}\".format(epoch, epoch_loss, val_loss))\n",
    "\n",
    "#plot the training run\n",
    "plt.plot(epoch_losses)\n",
    "plt.plot(val_losses, 'r')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylim(ymax=30, ymin=0)\n",
    "plt.draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eUo0_OJpxrV4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUo0_OJpxrV4",
    "outputId": "e6dec356-6685-4784-ab4d-fa14f8f21a1e"
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "model.eval()\n",
    "test_dir = \"./data/test_set\"\n",
    "test_rmsd = \"./data/test_rmsd_list.txt\"\n",
    "\n",
    "test_dataset = CGDataset(test_dir, test_rmsd)\n",
    "\n",
    "test_dataloader = dtl.pytorch.GraphDataLoader(val_dataset)\n",
    "\n",
    "test_losses = []\n",
    "for test_graph, test_label in test_dataloader:\n",
    "    test_pred = model(test_graph)\n",
    "    test_loss = F.smooth_l1_loss(test_pred, test_label).item()\n",
    "    test_losses.append(test_loss)\n",
    "    #print(\"Test Prediction: {:.4f}; true RMSD: {:.4f}; loss: {:.4f}\".format(test_pred.item(), test_label.item(), test_loss))\n",
    "\n",
    "test_mean = np.mean(test_losses)\n",
    "test_std = np.std(test_losses)\n",
    "print(\"Mean Test loss: \\t {:.4f}\".format(test_mean))\n",
    "print(\"Std. Dev. of Test loss:  {:.4f}\".format(test_std))\n",
    "print(\"Min loss: \\t \\t {:.4f}\".format(min(test_losses)))\n",
    "print(\"Max Loss: \\t \\t {:.4f}\".format(max(test_losses)))\n",
    "print(\"First Quantile: \\t {:.4f}\".format(np.quantile(test_losses, q=0.25)))\n",
    "print(\"Median: \\t \\t {:.4f}\".format(np.median(test_losses)))\n",
    "print(\"Third Quantile: \\t {:.4f}\".format(np.quantile(test_losses, q=0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a1064-da56-4fab-9ce5-fc0aa0952ef0",
   "metadata": {
    "id": "fb1a1064-da56-4fab-9ce5-fc0aa0952ef0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3d_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

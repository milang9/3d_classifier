{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a683650d-9deb-45f7-8d82-17d0203ec8f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a683650d-9deb-45f7-8d82-17d0203ec8f9",
        "outputId": "e611a436-f63b-458b-e0df-8d2f6475f6e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting forgi\n",
            "  Downloading forgi-2.1.2.tar.gz (26.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.8 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from forgi) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from forgi) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.20 in /usr/local/lib/python3.7/dist-packages (from forgi) (1.1.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from forgi) (0.16.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from forgi) (2.6.3)\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 38.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4 in /usr/local/lib/python3.7/dist-packages (from forgi) (1.4.4)\n",
            "Collecting logging_exceptions>=0.1.8\n",
            "  Downloading logging_exceptions-0.1.8-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from forgi) (0.29.24)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20->forgi) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20->forgi) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.20->forgi) (1.15.0)\n",
            "Building wheels for collected packages: forgi\n",
            "  Building wheel for forgi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for forgi: filename=forgi-2.1.2-cp37-cp37m-linux_x86_64.whl size=27791700 sha256=a734c51d794e52c8ae35e33627683aa02a156ecec35f6e7d2538eebb78629915\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/70/42/e333b0de9960f1b0cea75cf70a2db08290bcc713288a607e1a\n",
            "Successfully built forgi\n",
            "Installing collected packages: logging-exceptions, biopython, forgi\n",
            "Successfully installed biopython-1.79 forgi-2.1.2 logging-exceptions-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl\n",
        "#!DGLBACKEND=pytorch\n",
        "#!export $DGLBACKEND\n",
        "#import os\n",
        "#os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "#print(os.environ[\"DGLBACKEND\"])\n",
        "import dgl\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import torch as th\n",
        "\n",
        "!pip install forgi\n",
        "import forgi\n",
        "import forgi.graph.bulge_graph as fgb\n",
        "import forgi.threedee as ft\n",
        "import forgi.threedee.model.coarse_grain as ftmc\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "_MuRd2MIU0bz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MuRd2MIU0bz",
        "outputId": "34d4d5a6-6ab3-43cc-9c96-701d03f2cd15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(th.__version__)\n",
        "print(th.cuda.is_available())\n",
        "#!xz -d -v data.tar.xz\n",
        "!tar -xf data.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n2IxR1AxZdYn",
      "metadata": {
        "id": "n2IxR1AxZdYn"
      },
      "source": [
        "\n",
        "Ideas: \n",
        "*   use dgl.save_graph() to store a graph, so the structure can be used for several steps?\n",
        "*   use forgi.threedee.model.coarse_grain.CoarseGrainRNA.rotate() to rotate cg RNAs and see if the classification changes\n",
        "\n",
        "TODO:\n",
        "*  future --> find where ernwin writes/stores output of structure for each n steps\n",
        "*  finetune the model\n",
        "*  make larger batch of training data for testing\n",
        "*  include logger (maybe wandb?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "81afa4b6-ee92-4662-826b-de338ca595cf",
      "metadata": {
        "id": "81afa4b6-ee92-4662-826b-de338ca595cf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Graph Building\n",
        "\n",
        "#load coarse grain file\n",
        "def load_cg_file(file): \n",
        "    cg = ftmc.CoarseGrainRNA.from_bg_file(file) \n",
        "    c_dict = dict(cg.coords)\n",
        "    t_dict = dict(cg.twists)\n",
        "    coord_dict = {}\n",
        "    twist_dict = {}\n",
        "    for e in c_dict:\n",
        "        a = th.from_numpy(c_dict[e][0])\n",
        "        b = th.from_numpy(c_dict[e][1])\n",
        "        coord_dict[e] = a, b\n",
        "        if e in t_dict:\n",
        "            c = th.from_numpy(t_dict[e][0])\n",
        "            d = th.from_numpy(t_dict[e][1])\n",
        "            twist_dict[e] = c, d\n",
        "        \n",
        "    # Get elements and neighbours:\n",
        "    connections = {}\n",
        "    for elem in cg.sorted_element_iterator():\n",
        "        neighbours = cg.connections(elem)\n",
        "        if elem not in connections:\n",
        "            connections[elem] = cg.connections(elem)\n",
        "    return coord_dict, twist_dict, connections\n",
        "\n",
        "def build_dgl_graph(coord_dict, twist_dict, connections):\n",
        "    #dictionary to convert type\n",
        "    type_transl = {\n",
        "        \"h\": [1, 0, 0, 0, 0, 0],\n",
        "        \"i\": [0, 1, 0, 0, 0, 0],\n",
        "        \"m\": [0, 0, 1, 0, 0, 0],\n",
        "        \"s\": [0, 0, 0, 1, 0, 0],\n",
        "        \"f\": [0, 0, 0, 0, 1, 0],\n",
        "        \"t\": [0, 0, 0, 0, 0, 1]\n",
        "    } \n",
        "\n",
        "    #encode nodes numerically for dgl graph\n",
        "    num_graph = {}\n",
        "    elem_count = {}\n",
        "    for num, n in enumerate(sorted(connections)):\n",
        "        num_graph[n] = num\n",
        "        if n[0] not in elem_count:\n",
        "            elem_count[n[0]] = 1\n",
        "        else:\n",
        "            elem_count[n[0]] += 1\n",
        "\n",
        "    #build graph and edges\n",
        "    u = []\n",
        "    v = []\n",
        "    for node in connections:\n",
        "        for c in connections[node]:\n",
        "            u.append(num_graph[node])\n",
        "            v.append(num_graph[c])\n",
        "\n",
        "    graph = dgl.graph((th.tensor(u), th.tensor(v)))\n",
        "\n",
        "    #initialise node attributes\n",
        "    graph.ndata[\"type\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32)\n",
        "    graph.ndata[\"coord\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32) #seperate coords into 2 sets of 3, so that the information of start and end is added?\n",
        "    graph.ndata[\"twist\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32)\n",
        "\n",
        "    for elem in connections:\n",
        "        graph.ndata[\"type\"][num_graph[elem]] = th.tensor(type_transl[elem[0]], dtype=th.float32) \n",
        "        graph.ndata[\"coord\"][num_graph[elem]] = th.tensor(np.concatenate(coord_dict[elem]), dtype=th.float32)\n",
        "        if elem in twist_dict:\n",
        "            graph.ndata[\"twist\"][num_graph[elem]] = th.tensor(np.concatenate(twist_dict[elem]), dtype=th.float32)\n",
        "  \n",
        "    return graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "65010c86-88ed-4940-b36f-5af93ea50a6d",
      "metadata": {
        "id": "65010c86-88ed-4940-b36f-5af93ea50a6d"
      },
      "outputs": [],
      "source": [
        "#create a dict with name and rmsd as labels\n",
        "def get_rmsd_dict(rmsd_list):\n",
        "    #rmsd_list = \"./play_set/RMSD_list.txt\"\n",
        "    rmsd_dict = {}\n",
        "    with open(rmsd_list, \"r\") as fh:\n",
        "        for line in fh.readlines():\n",
        "            name, rmsd = (line.rstrip()).split(\"\\t\")\n",
        "            rmsd_dict[name] = float(rmsd)\n",
        "    return rmsd_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ZU_znRFH141n",
      "metadata": {
        "id": "ZU_znRFH141n"
      },
      "outputs": [],
      "source": [
        "#Graph Dataset Class\n",
        "#TODO: adapt, so it can stand alone\n",
        "\n",
        "from dgl.data import DGLDataset\n",
        "class CGDataset(DGLDataset):\n",
        "    def __init__(self, directory, rmsd_list):\n",
        "        self.file_path = directory\n",
        "        self.rmsd_list = rmsd_list\n",
        "        super(CGDataset, self).__init__(name=\"cgRNA\")\n",
        "        \n",
        "        \n",
        "    def process(self):\n",
        "        self.graphs = []\n",
        "        rmsd_dict = get_rmsd_dict(self.rmsd_list)\n",
        "        self.labels = []\n",
        "        \n",
        "        files = []\n",
        "        filenames = next(os.walk(self.file_path), (None, None, []))[2]\n",
        "\n",
        "        for file in filenames:\n",
        "            if file.endswith(\".cg\"):\n",
        "                files.append(file)\n",
        "                self.labels.append(rmsd_dict[file])\n",
        "\n",
        "        for struc in files:\n",
        "            coord_dict, twist_dict, connections = load_cg_file(os.path.join(self.file_path, struc))\n",
        "            graph = build_dgl_graph(coord_dict, twist_dict, connections)\n",
        "            self.graphs.append(build_dgl_graph(coord_dict, twist_dict, connections))\n",
        "\n",
        "        self.labels = th.tensor(self.labels)\n",
        "  \n",
        "    def __getitem__(self, i):\n",
        "        return self.graphs[i], self.labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "    \n",
        "    #def save(self):\n",
        "     #   dgl.save_graphs(\"./play_set/training_cg_graphs.dgl\", self.graphs, labels=self.labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "j4O333bvAQ1V",
      "metadata": {
        "id": "j4O333bvAQ1V"
      },
      "outputs": [],
      "source": [
        "#Dataloading\n",
        "import dgl.dataloading as dtl\n",
        "\n",
        "b_size = 50\n",
        "\n",
        "#load from cg files directly\n",
        "\n",
        "training_dir = \"./data/training_set\"\n",
        "rmsd_list = \"./data/train_rmsd_list.txt\"\n",
        "\n",
        "training_dataset = CGDataset(training_dir, rmsd_list)\n",
        "\n",
        "#add randomisation as in Defining Data Loader from https://docs.dgl.ai/tutorials/blitz/5_graph_classification.html\n",
        "train_dataloader = dtl.pytorch.GraphDataLoader(training_dataset, batch_size=b_size, shuffle=True) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bq8dpernrILf",
      "metadata": {
        "id": "bq8dpernrILf"
      },
      "outputs": [],
      "source": [
        "#Validation set\n",
        "val_dir = \"./data/val_set\"\n",
        "val_rmsd = \"./data/val_rmsd_list.txt\"\n",
        "\n",
        "val_dataset = CGDataset(val_dir, val_rmsd)\n",
        "\n",
        "val_dataloader = dtl.pytorch.GraphDataLoader(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "35ebadfa-2a66-46ea-ac4d-e6ff32b2205b",
      "metadata": {
        "id": "35ebadfa-2a66-46ea-ac4d-e6ff32b2205b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91faabfb-249f-4f47-a431-3fa756f17313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes=17, num_edges=34,\n",
            "      ndata_schemes={'type': Scheme(shape=(6,), dtype=torch.float32), 'coord': Scheme(shape=(6,), dtype=torch.float32), 'twist': Scheme(shape=(6,), dtype=torch.float32)}\n",
            "      edata_schemes={})\n"
          ]
        }
      ],
      "source": [
        "#perhaps a module to coarsen/pool graph nodes\n",
        "test_g = training_dataset[0][0]\n",
        "\n",
        "dgl.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fec1a7b3-8adc-45a3-83ab-7cb02f31a4af",
      "metadata": {
        "id": "fec1a7b3-8adc-45a3-83ab-7cb02f31a4af"
      },
      "outputs": [],
      "source": [
        "#Model\n",
        "import dgl.nn as dglnn\n",
        "from dgl.nn import GraphConv\n",
        "import torch.nn.functional as F\n",
        "import dgl.function as fn\n",
        "from dgl.geometry import neighbor_matching\n",
        "\n",
        "# feed the 3 different node attributes one after the other though the first layer? like in https://discuss.dgl.ai/t/getting-started-with-multiple-node-features-in-homogenous-graph/919/2\n",
        "# condense the 3 node attributes down to 1? see point above\n",
        "\n",
        "\n",
        "#Coarse Grain RNA Classifier Model\n",
        "class CG_Classifier(th.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        self.c = 0\n",
        "        super(CG_Classifier, self).__init__()\n",
        "        \n",
        "        '''\n",
        "        self.conv1 = GraphConv(6, 48, activation=F.relu)\n",
        "        self.conv2 = GraphConv(48, 24, activation=F.relu)\n",
        "        self.conv3 = GraphConv(24, 12, activation=F.relu)\n",
        "        '''\n",
        "        self.conv1 = dglnn.TAGConv(6, 128, k=2, activation=F.relu)\n",
        "        self.conv2 = dglnn.TAGConv(128, 64, k=2, activation=F.relu)\n",
        "        self.conv3 = dglnn.TAGConv(64, 32, k=1, activation=F.relu)\n",
        "        \n",
        "        self.max_pool = dglnn.SumPooling() #dgl.nn.MaxPooling()\n",
        "        #self.diffpool = dgl.nn.\n",
        "\n",
        "        self.sage_conv1 = dglnn.SAGEConv(32*num_features, 24, 'pool') \n",
        "        self.sage_conv2 = dglnn.SAGEConv(24, 20, 'pool')\n",
        "        self.sage_conv3 = dglnn.SAGEConv(20, 16, 'pool')\n",
        "\n",
        "        self.dense1 = th.nn.Linear(16, 512)\n",
        "        self.dense2 = th.nn.Linear(512, 512)\n",
        "        self.dense3 = th.nn.Linear(512, 512)\n",
        "        self.classify = th.nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, g):\n",
        "        nt = g.ndata[\"type\"]\n",
        "        nc = g.ndata[\"coord\"]\n",
        "        nw = g.ndata[\"twist\"]\n",
        "        \n",
        "        nt = self.conv1(g, nt)\n",
        "        nt = self.conv2(g, nt)\n",
        "        nt = self.conv3(g, nt)\n",
        "\n",
        "        nc = self.conv1(g, nc)\n",
        "        nc = self.conv2(g, nc)\n",
        "        nc = self.conv3(g, nc)\n",
        "        \n",
        "        nw = self.conv1(g, nw)\n",
        "        nw = self.conv2(g, nw)\n",
        "        nw = self.conv3(g, nw)\n",
        "\n",
        "        #TODO: Modify the readout function (maybe with the local scope below)\n",
        "        \n",
        "        #use pooling to have still a graph representation, after 2 layers of seperate conv\n",
        "        #--> let conv run over the pooled graph\n",
        "        #with g.local_scope():\n",
        "        #    print(th.cat((nt, nc, nw), 0))\n",
        "        #    g.ndata[\"combi\"] = th.cat((nt, nc, nw), 0)\n",
        "        #    tcw = g.ndata[\"combi\"]\n",
        "        #    tcw = self.conv3(g, tcw)\n",
        "        #    tcw = self.max_pool(g, tcw)\n",
        "            #tcw = th.cat((nt, nc, nw), 1) #use this for graph gen and again graph con\n",
        "\n",
        "\n",
        "        #TODO: add readout function\n",
        "        #      how to best use pooling?\n",
        "                \n",
        "        #g.update_all(fn.copy_u(src=\"combi\", out=\"pool1\"), fn.max()) #find out what could be target and destination for fn.copy_u\n",
        "        #combi = g.ndata[\"pool1\"]\n",
        "        \n",
        "        g.ndata[\"combi\"] = th.cat((nt, nc, nw), 1) #tcw\n",
        "\n",
        "        combi = g.ndata[\"combi\"]\n",
        "        combi = self.sage_conv1(g, combi)\n",
        "        combi = self.sage_conv2(g, combi)\n",
        "        combi = self.sage_conv3(g, combi)\n",
        "        combi = self.max_pool(g, combi)\n",
        "\n",
        "        tcw_mean = combi.mean(dim=0) #dgl.mean_nodes(g, combi)\n",
        "        #if self.c == 0:\n",
        "        #    print(tcw_mean)\n",
        "            \n",
        "        #is the mean the right approach?\n",
        "        #tcw_mean = tcw.mean(dim=0)\n",
        "        tcw_mean = self.dense1(tcw_mean)\n",
        "        tcw_mean = self.dense2(tcw_mean)\n",
        "        tcw_mean = self.dense3(tcw_mean)\n",
        "        #if self.c == 0:\n",
        "        #    print(tcw_mean)\n",
        "        #    self.c = 1\n",
        "        \n",
        "        return self.classify(tcw_mean)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2943be90-6647-4c26-b967-a2563a142877",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2943be90-6647-4c26-b967-a2563a142877",
        "outputId": "c21cd277-f7c2-44e7-fda6-a42d94bf3281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Training loss 993.4221, Validation loss 363.7846\n",
            "Epoch 5: Training loss 13.4250, Validation loss 11.2177\n",
            "Epoch 10: Training loss 8.0927, Validation loss 7.7191\n",
            "Epoch 15: Training loss 6.4795, Validation loss 8.3870\n",
            "Epoch 20: Training loss 6.0631, Validation loss 6.4917\n",
            "Epoch 25: Training loss 4.5109, Validation loss 6.2523\n",
            "Epoch 30: Training loss 4.3163, Validation loss 5.6856\n",
            "Epoch 35: Training loss 4.1666, Validation loss 5.7101\n",
            "Epoch 40: Training loss 3.3477, Validation loss 5.3909\n",
            "Epoch 45: Training loss 3.0856, Validation loss 6.0991\n",
            "Epoch 50: Training loss 2.9557, Validation loss 5.6979\n",
            "Epoch 55: Training loss 2.3205, Validation loss 5.6487\n",
            "Epoch 60: Training loss 2.5179, Validation loss 5.4975\n",
            "Epoch 65: Training loss 2.4215, Validation loss 5.3462\n",
            "Epoch 70: Training loss 1.9194, Validation loss 5.4788\n",
            "Epoch 75: Training loss 1.6811, Validation loss 5.0806\n",
            "Epoch 80: Training loss 1.7207, Validation loss 5.3591\n",
            "Epoch 85: Training loss 1.7684, Validation loss 5.5563\n",
            "Epoch 90: Training loss 1.6050, Validation loss 5.2600\n",
            "Epoch 95: Training loss 1.8786, Validation loss 4.9391\n",
            "Epoch 100: Training loss 1.2655, Validation loss 5.0964\n",
            "Epoch 105: Training loss 1.6205, Validation loss 5.4676\n",
            "Epoch 110: Training loss 1.3242, Validation loss 4.5601\n",
            "Epoch 115: Training loss 1.2572, Validation loss 4.6959\n",
            "Epoch 120: Training loss 1.1034, Validation loss 4.9052\n",
            "Epoch 125: Training loss 0.7821, Validation loss 4.7643\n",
            "Epoch 130: Training loss 0.8310, Validation loss 4.8941\n",
            "Epoch 135: Training loss 1.0945, Validation loss 4.7431\n",
            "Epoch 140: Training loss 0.7604, Validation loss 4.5617\n",
            "Epoch 145: Training loss 1.2530, Validation loss 4.8058\n",
            "Epoch 150: Training loss 1.4930, Validation loss 4.7499\n",
            "Epoch 155: Training loss 0.9404, Validation loss 4.3937\n",
            "Epoch 160: Training loss 1.2510, Validation loss 4.9522\n",
            "Epoch 165: Training loss 1.0026, Validation loss 4.7882\n",
            "Epoch 170: Training loss 0.8955, Validation loss 4.8073\n",
            "Epoch 175: Training loss 1.0864, Validation loss 4.8616\n",
            "Epoch 180: Training loss 0.9424, Validation loss 4.8586\n",
            "Epoch 185: Training loss 0.8925, Validation loss 4.6517\n",
            "Epoch 190: Training loss 1.0148, Validation loss 4.7126\n",
            "Epoch 195: Training loss 0.6093, Validation loss 4.7101\n",
            "Epoch 200: Training loss 0.8658, Validation loss 4.5861\n",
            "Epoch 205: Training loss 1.0798, Validation loss 4.5859\n",
            "Epoch 210: Training loss 0.9914, Validation loss 4.4844\n",
            "Epoch 215: Training loss 0.6136, Validation loss 4.4308\n",
            "Epoch 220: Training loss 0.8218, Validation loss 4.4303\n",
            "Epoch 225: Training loss 0.7635, Validation loss 4.4753\n",
            "Epoch 230: Training loss 0.7248, Validation loss 4.7536\n",
            "Epoch 235: Training loss 1.0394, Validation loss 4.8863\n",
            "Epoch 240: Training loss 1.0395, Validation loss 4.7442\n",
            "Epoch 245: Training loss 0.6512, Validation loss 4.2635\n",
            "Epoch 250: Training loss 0.6305, Validation loss 4.3741\n",
            "Epoch 255: Training loss 0.7136, Validation loss 4.4817\n",
            "Epoch 260: Training loss 0.6073, Validation loss 4.5400\n",
            "Epoch 265: Training loss 0.6963, Validation loss 4.5813\n",
            "Epoch 270: Training loss 0.6621, Validation loss 4.5040\n",
            "Epoch 275: Training loss 0.6648, Validation loss 4.1350\n",
            "Epoch 280: Training loss 0.4258, Validation loss 4.3092\n",
            "Epoch 285: Training loss 0.4571, Validation loss 4.1868\n",
            "Epoch 290: Training loss 0.9712, Validation loss 4.5071\n",
            "Epoch 295: Training loss 1.1281, Validation loss 4.3423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.draw>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Training\n",
        "#TODO:  build a deeper neural network and refine pooling\n",
        "#       tinker with hyperparameters\n",
        "\n",
        "model = CG_Classifier(\n",
        "    num_features=3 #len(graph.ndata)\n",
        ")\n",
        "\n",
        "opt = th.optim.Adam(model.parameters(), lr=0.001)\n",
        "model.train()\n",
        "#th.autograd.set_detect_anomaly(True)\n",
        "epochs = 300\n",
        "\n",
        "#val setup\n",
        "val_losses = []\n",
        "\n",
        "#training setup\n",
        "epoch_losses = []\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for iter, (batched_graph, batch_labels) in enumerate(train_dataloader):\n",
        "        l = []\n",
        "        for graph in dgl.unbatch(batched_graph): #ist there a way to use directly the batched_graph object?\n",
        "            pred = model(graph)\n",
        "            l.append(pred)\n",
        "        \n",
        "        logits = th.cat(l)\n",
        "        loss = F.smooth_l1_loss(logits, batch_labels, reduction='mean')\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        epoch_loss += loss.detach().item()\n",
        "    \n",
        "    epoch_loss /= (iter + 1)\n",
        "    epoch_losses.append(epoch_loss)\n",
        "\n",
        "    #val setup\n",
        "    val_loss = 0\n",
        "    for i, (v_graph, v_label) in enumerate(val_dataloader):\n",
        "        val_pred = model(v_graph)\n",
        "        v_loss = F.smooth_l1_loss(val_pred, v_label, reduction='mean')\n",
        "        val_loss += v_loss.detach().item()\n",
        "\n",
        "    val_loss /= (i + 1)\n",
        "    \n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"Epoch {}: Training loss {:.4f}, Validation loss {:.4f}\".format(epoch, epoch_loss, val_loss))\n",
        "\n",
        "#plot the training run\n",
        "plt.plot(epoch_losses)\n",
        "plt.plot(val_losses, 'r')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylim(ymax=30, ymin=0)\n",
        "plt.draw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "eUo0_OJpxrV4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUo0_OJpxrV4",
        "outputId": "2b4717fb-92ff-48a4-8d56-b227c9de0995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Test loss: \t 4.2344\n",
            "Std. Dev. of Test loss:  4.2930\n",
            "Min loss: \t \t 0.0013\n",
            "First Quantile: \t 1.1563\n",
            "Median: \t \t 2.9519\n",
            "Third Quantile: \t 6.1240\n",
            "Max Loss: \t \t 22.5539\n"
          ]
        }
      ],
      "source": [
        "#Test\n",
        "\n",
        "model.eval()\n",
        "test_dir = \"./data/test_set\"\n",
        "test_rmsd = \"./data/test_rmsd_list.txt\"\n",
        "\n",
        "test_dataset = CGDataset(test_dir, test_rmsd)\n",
        "\n",
        "test_dataloader = dtl.pytorch.GraphDataLoader(val_dataset)\n",
        "\n",
        "test_losses = []\n",
        "for test_graph, test_label in test_dataloader:\n",
        "    test_pred = model(test_graph)\n",
        "    test_loss = F.smooth_l1_loss(test_pred, test_label).item()\n",
        "    test_losses.append(test_loss)\n",
        "    #print(\"Test Prediction: {:.4f}; true RMSD: {:.4f}; loss: {:.4f}\".format(test_pred.item(), test_label.item(), test_loss))\n",
        "\n",
        "test_mean = np.mean(test_losses)\n",
        "test_std = np.std(test_losses)\n",
        "print(\"Mean Test loss: \\t {:.4f}\".format(test_mean))\n",
        "print(\"Std. Dev. of Test loss:  {:.4f}\".format(test_std))\n",
        "print(\"Min loss: \\t \\t {:.4f}\".format(min(test_losses)))\n",
        "print(\"First Quantile: \\t {:.4f}\".format(np.quantile(test_losses, q=0.25)))\n",
        "print(\"Median: \\t \\t {:.4f}\".format(np.median(test_losses)))\n",
        "print(\"Third Quantile: \\t {:.4f}\".format(np.quantile(test_losses, q=0.75)))\n",
        "print(\"Max Loss: \\t \\t {:.4f}\".format(max(test_losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb1a1064-da56-4fab-9ce5-fc0aa0952ef0",
      "metadata": {
        "id": "fb1a1064-da56-4fab-9ce5-fc0aa0952ef0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "3d_classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683650d-9deb-45f7-8d82-17d0203ec8f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a683650d-9deb-45f7-8d82-17d0203ec8f9",
    "outputId": "8448f4fe-9bbc-434a-baab-24ce6537aa2f"
   },
   "outputs": [],
   "source": [
    "#!pip install dgl\n",
    "#!DGLBACKEND=pytorch\n",
    "#!export $DGLBACKEND\n",
    "#import os\n",
    "#os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "#print(os.environ[\"DGLBACKEND\"])\n",
    "import dgl\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import torch as th\n",
    "\n",
    "#!pip install forgi\n",
    "import forgi\n",
    "import forgi.graph.bulge_graph as fgb\n",
    "import forgi.threedee as ft\n",
    "import forgi.threedee.model.coarse_grain as ftmc\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_MuRd2MIU0bz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MuRd2MIU0bz",
    "outputId": "d66ab529-6cd4-42ba-c469-b020e7a01fdd"
   },
   "outputs": [],
   "source": [
    "print(th.__version__)\n",
    "print(th.cuda.is_available())\n",
    "#!xz -d -v data.tar.xz\n",
    "#!tar -xf data.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n2IxR1AxZdYn",
   "metadata": {
    "id": "n2IxR1AxZdYn"
   },
   "source": [
    "\n",
    "Ideas: \n",
    "*   use dgl.save_graph() to store a graph, so the structure can be used for several steps?\n",
    "*   use forgi.threedee.model.coarse_grain.CoarseGrainRNA.rotate() to rotate cg RNAs and see if the classification changes\n",
    "\n",
    "TODO:\n",
    "*  future --> find where ernwin writes/stores output of structure for each n steps\n",
    "*  finetune the model\n",
    "*  make larger batch of training data for testing\n",
    "*  include logger (maybe wandb?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afa4b6-ee92-4662-826b-de338ca595cf",
   "metadata": {
    "id": "81afa4b6-ee92-4662-826b-de338ca595cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Graph Building\n",
    "\n",
    "#load coarse grain file\n",
    "def load_cg_file(file): \n",
    "    cg = ftmc.CoarseGrainRNA.from_bg_file(file) \n",
    "    c_dict = dict(cg.coords)\n",
    "    t_dict = dict(cg.twists)\n",
    "    coord_dict = {}\n",
    "    twist_dict = {}\n",
    "    for e in c_dict:\n",
    "        a = th.from_numpy(c_dict[e][0])\n",
    "        b = th.from_numpy(c_dict[e][1])\n",
    "        coord_dict[e] = a, b\n",
    "        if e in t_dict:\n",
    "            c = th.from_numpy(t_dict[e][0])\n",
    "            d = th.from_numpy(t_dict[e][1])\n",
    "            twist_dict[e] = c, d\n",
    "        \n",
    "    # Get elements and neighbours:\n",
    "    connections = {}\n",
    "    for elem in cg.sorted_element_iterator():\n",
    "        neighbours = cg.connections(elem)\n",
    "        if elem not in connections:\n",
    "            connections[elem] = cg.connections(elem)\n",
    "    return coord_dict, twist_dict, connections\n",
    "\n",
    "def build_dgl_graph(coord_dict, twist_dict, connections):\n",
    "    #dictionary to convert type\n",
    "    type_transl = {\n",
    "        \"h\": [1, 0, 0, 0, 0, 0],\n",
    "        \"i\": [0, 1, 0, 0, 0, 0],\n",
    "        \"m\": [0, 0, 1, 0, 0, 0],\n",
    "        \"s\": [0, 0, 0, 1, 0, 0],\n",
    "        \"f\": [0, 0, 0, 0, 1, 0],\n",
    "        \"t\": [0, 0, 0, 0, 0, 1]\n",
    "    } \n",
    "\n",
    "    #encode nodes numerically for dgl graph\n",
    "    num_graph = {}\n",
    "    elem_count = {}\n",
    "    for num, n in enumerate(sorted(connections)):\n",
    "        num_graph[n] = num\n",
    "        if n[0] not in elem_count:\n",
    "            elem_count[n[0]] = 1\n",
    "        else:\n",
    "            elem_count[n[0]] += 1\n",
    "\n",
    "    #build graph and edges\n",
    "    u = []\n",
    "    v = []\n",
    "    for node in connections:\n",
    "        for c in connections[node]:\n",
    "            u.append(num_graph[node])\n",
    "            v.append(num_graph[c])\n",
    "\n",
    "    graph = dgl.graph((th.tensor(u), th.tensor(v)))\n",
    "\n",
    "    #initialise node attributes\n",
    "    graph.ndata[\"type\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32)\n",
    "    graph.ndata[\"coord\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32) #seperate coords into 2 sets of 3, so that the information of start and end is added?\n",
    "    graph.ndata[\"twist\"] = th.zeros(graph.num_nodes(), 6, dtype=th.float32)\n",
    "\n",
    "    for elem in connections:\n",
    "        graph.ndata[\"type\"][num_graph[elem]] = th.tensor(type_transl[elem[0]], dtype=th.float32) \n",
    "        graph.ndata[\"coord\"][num_graph[elem]] = th.tensor(np.concatenate(coord_dict[elem]), dtype=th.float32)\n",
    "        if elem in twist_dict:\n",
    "            graph.ndata[\"twist\"][num_graph[elem]] = th.tensor(np.concatenate(twist_dict[elem]), dtype=th.float32)\n",
    "  \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65010c86-88ed-4940-b36f-5af93ea50a6d",
   "metadata": {
    "id": "65010c86-88ed-4940-b36f-5af93ea50a6d"
   },
   "outputs": [],
   "source": [
    "#create a dict with name and rmsd as labels\n",
    "def get_rmsd_dict(rmsd_list):\n",
    "    #rmsd_list = \"./play_set/RMSD_list.txt\"\n",
    "    rmsd_dict = {}\n",
    "    with open(rmsd_list, \"r\") as fh:\n",
    "        for line in fh.readlines():\n",
    "            name, rmsd = (line.rstrip()).split(\"\\t\")\n",
    "            rmsd_dict[name] = float(rmsd)\n",
    "    return rmsd_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZU_znRFH141n",
   "metadata": {
    "id": "ZU_znRFH141n"
   },
   "outputs": [],
   "source": [
    "#Graph Dataset Class\n",
    "#TODO: adapt, so it can stand alone\n",
    "\n",
    "from dgl.data import DGLDataset\n",
    "class CGDataset(DGLDataset):\n",
    "    def __init__(self, directory, rmsd_list):\n",
    "        self.file_path = directory\n",
    "        self.rmsd_list = rmsd_list\n",
    "        super(CGDataset, self).__init__(name=\"cgRNA\")\n",
    "        \n",
    "        \n",
    "    def process(self):\n",
    "        self.graphs = []\n",
    "        rmsd_dict = get_rmsd_dict(self.rmsd_list)\n",
    "        self.labels = []\n",
    "        \n",
    "        files = []\n",
    "        filenames = next(os.walk(self.file_path), (None, None, []))[2]\n",
    "\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".cg\"):\n",
    "                files.append(file)\n",
    "                self.labels.append(rmsd_dict[file])\n",
    "\n",
    "        for struc in files:\n",
    "            coord_dict, twist_dict, connections = load_cg_file(os.path.join(self.file_path, struc))\n",
    "            graph = build_dgl_graph(coord_dict, twist_dict, connections)\n",
    "            self.graphs.append(build_dgl_graph(coord_dict, twist_dict, connections))\n",
    "\n",
    "        self.labels = th.tensor(self.labels)\n",
    "  \n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    #def save(self):\n",
    "     #   dgl.save_graphs(\"./play_set/training_cg_graphs.dgl\", self.graphs, labels=self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j4O333bvAQ1V",
   "metadata": {
    "id": "j4O333bvAQ1V"
   },
   "outputs": [],
   "source": [
    "#Dataloading\n",
    "import dgl.dataloading as dtl\n",
    "\n",
    "b_size = 50\n",
    "\n",
    "#load from cg files directly\n",
    "\n",
    "training_dir = \"./data/training_set\"\n",
    "rmsd_list = \"./data/train_rmsd_list.txt\"\n",
    "\n",
    "training_dataset = CGDataset(training_dir, rmsd_list)\n",
    "\n",
    "#add randomisation as in Defining Data Loader from https://docs.dgl.ai/tutorials/blitz/5_graph_classification.html\n",
    "train_dataloader = dtl.pytorch.GraphDataLoader(training_dataset, batch_size=b_size, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bq8dpernrILf",
   "metadata": {
    "id": "bq8dpernrILf"
   },
   "outputs": [],
   "source": [
    "#Validation set\n",
    "val_dir = \"./data/val_set\"\n",
    "val_rmsd = \"./data/val_rmsd_list.txt\"\n",
    "\n",
    "val_dataset = CGDataset(val_dir, val_rmsd)\n",
    "\n",
    "val_dataloader = dtl.pytorch.GraphDataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebadfa-2a66-46ea-ac4d-e6ff32b2205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perhaps a module to coarsen/pool graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fec1a7b3-8adc-45a3-83ab-7cb02f31a4af",
   "metadata": {
    "id": "fec1a7b3-8adc-45a3-83ab-7cb02f31a4af"
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "import dgl.nn as dglnn\n",
    "from dgl.nn import GraphConv\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.geometry import neighbor_matching\n",
    "\n",
    "# feed the 3 different node attributes one after the other though the first layer? like in https://discuss.dgl.ai/t/getting-started-with-multiple-node-features-in-homogenous-graph/919/2\n",
    "# condense the 3 node attributes down to 1? see point above\n",
    "\n",
    "\n",
    "#Coarse Grain RNA Classifier Model\n",
    "class CG_Classifier(th.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        self.c = 0\n",
    "        super(CG_Classifier, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        self.conv1 = GraphConv(6, 48, activation=F.relu)\n",
    "        self.conv2 = GraphConv(48, 24, activation=F.relu)\n",
    "        self.conv3 = GraphConv(24, 12, activation=F.relu)\n",
    "        '''\n",
    "        self.conv1 = dglnn.TAGConv(6, 64, k=2, activation=F.relu)\n",
    "        self.conv2 = dglnn.TAGConv(64, 32, k=2, activation=F.relu)\n",
    "        self.conv3 = dglnn.TAGConv(32, 24, k=1, activation=F.relu)\n",
    "        \n",
    "        self.max_pool = dgl.nn.MaxPooling()\n",
    "        \n",
    "        self.sage_conv1 = dglnn.SAGEConv(24*num_features, 20, 'pool') \n",
    "        self.sage_conv2 = dglnn.SAGEConv(20, 16, 'pool')\n",
    "\n",
    "        self.dense1 = th.nn.Linear(512, 512) #th.nn.ReLU(th.nn.Linear(16, 512))\n",
    "        self.dense2 = th.nn.Linear(512, 512) #th.nn.ReLU(th.nn.Linear(512, 512))\n",
    "        self.dense3 = th.nn.Linear(512, 512) #th.nn.ReLU(th.nn.Linear(512, 512))\n",
    "        self.classify = th.nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        nt = g.ndata[\"type\"]\n",
    "        nc = g.ndata[\"coord\"]\n",
    "        nw = g.ndata[\"twist\"]\n",
    "        \n",
    "        nt = self.conv1(g, nt)\n",
    "        #nt = self.max_pool(g, nt)\n",
    "        nt = self.conv2(g, nt)\n",
    "        #nt = self.max_pool(g, nt)\n",
    "        nt = self.conv3(g, nt)\n",
    "        #nt = self.max_pool(g, nt)\n",
    "\n",
    "        nc = self.conv1(g, nc)\n",
    "        #nc = self.max_pool(g, nc)\n",
    "        nc = self.conv2(g, nc)\n",
    "        #nc = self.max_pool(g, nc)\n",
    "        nc = self.conv3(g, nc)\n",
    "        #nc = self.max_pool(g, nc)\n",
    "        \n",
    "        nw = self.conv1(g, nw)\n",
    "        #nw = self.max_pool(g, nw)\n",
    "        nw = self.conv2(g, nw)\n",
    "        #nw = self.max_pool(g, nw)\n",
    "        nw = self.conv3(g, nw)\n",
    "        #nw = self.max_pool(g, nw)\n",
    "\n",
    "        #TODO: Modify the readout function (maybe with the local scope below)\n",
    "        \n",
    "        #use pooling to have still a graph representation, after 2 layers of seperate conv\n",
    "        #--> let conv run over the pooled graph\n",
    "        #with g.local_scope():\n",
    "        #    print(th.cat((nt, nc, nw), 0))\n",
    "        #    g.ndata[\"combi\"] = th.cat((nt, nc, nw), 0)\n",
    "        #    tcw = g.ndata[\"combi\"]\n",
    "        #    tcw = self.conv3(g, tcw)\n",
    "        #    tcw = self.max_pool(g, tcw)\n",
    "            #tcw = th.cat((nt, nc, nw), 1) #use this for graph gen and again graph con\n",
    "\n",
    "\n",
    "        #TODO: add readout function\n",
    "        #      how to best use pooling?\n",
    "        #tcw = th.cat((nt, nc, nw), 1)\n",
    "        #if self.c == 0:\n",
    "         #   print(tcw)\n",
    "        g.ndata[\"combi\"] = th.cat((nt, nc, nw), 1) #tcw\n",
    "        #if self.c == 0:\n",
    "            #print(g.ndata[\"combi\"])\n",
    "            #print(g)\n",
    "            #new_g = neighbor_matching(g)\n",
    "            #print(new_g)\n",
    "        combi = g.ndata[\"combi\"]\n",
    "        combi = self.sage_conv1(g, combi)\n",
    "        \n",
    "        #g.update_all(fn.copy_u(src=\"combi\", out=\"pool1\"), fn.max()) #find out what could be target and destination for fn.copy_u\n",
    "        #combi = g.ndata[\"pool1\"]\n",
    "        \n",
    "        combi = self.sage_conv2(g, combi)\n",
    "        combi = self.max_pool(g, combi)\n",
    "\n",
    "        tcw_mean = combi#.mean(dim=0) #dgl.mean_nodes(g, combi)\n",
    "        if self.c == 0:\n",
    "            print(tcw_mean)\n",
    "            \n",
    "        #is the mean the right approach?\n",
    "        #tcw_mean = tcw.mean(dim=0)\n",
    "        tcw_mean = self.dense1(tcw_mean)\n",
    "        if self.c == 0:\n",
    "            print(tcw_mean)\n",
    "        tcw_mean = self.dense2(tcw_mean)\n",
    "        if self.c == 0:\n",
    "            print(tcw_mean)\n",
    "        tcw_mean = self.dense3(tcw_mean)\n",
    "        if self.c == 0:\n",
    "            print(tcw_mean)\n",
    "            self.c = 1\n",
    "        \n",
    "        return self.classify(tcw_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2943be90-6647-4c26-b967-a2563a142877",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "2943be90-6647-4c26-b967-a2563a142877",
    "outputId": "a9353659-684a-43c7-9a04-8c960be46684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 71.7381,   1.7913, -52.9559, -29.7088, -21.9408,  85.0219,  75.9519,\n",
      "         -40.4645, -28.4156,  -9.5954,  -5.7817, -23.6151,  -4.8492,  78.4227,\n",
      "          60.0771,  18.6749]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x16 and 512x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10168/147789845.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#ist there a way to use directly the batched_graph object?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10168/167898858.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m#is the mean the right approach?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#tcw_mean = tcw.mean(dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtcw_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcw_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcw_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x16 and 512x512)"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "#TODO:  build a deeper neural network and refine pooling\n",
    "#       tinker with hyperparameters\n",
    "\n",
    "model = CG_Classifier(\n",
    "    num_features=3 #len(graph.ndata)\n",
    ")\n",
    "\n",
    "opt = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "th.autograd.set_detect_anomaly(True)\n",
    "epochs = 100\n",
    "\n",
    "#val setup\n",
    "val_losses = []\n",
    "\n",
    "#training setup\n",
    "epoch_losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for iter, (batched_graph, batch_labels) in enumerate(train_dataloader):\n",
    "        l = []\n",
    "        for graph in dgl.unbatch(batched_graph): #ist there a way to use directly the batched_graph object?\n",
    "            pred = model(graph)\n",
    "            l.append(pred)\n",
    "        \n",
    "        logits = th.cat(l)\n",
    "        loss = F.smooth_l1_loss(logits, batch_labels, reduction='mean')\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    \n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    #val setup\n",
    "    val_loss = 0\n",
    "    for i, (v_graph, v_label) in enumerate(val_dataloader):\n",
    "        val_pred = model(v_graph)\n",
    "        v_loss = F.smooth_l1_loss(val_pred, v_label, reduction='mean')\n",
    "        val_loss += v_loss.detach().item()\n",
    "\n",
    "    val_loss /= (i + 1)\n",
    "    \n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch {}: Training loss {:.4f}, Validation loss {:.4f}\".format(epoch, epoch_loss, val_loss))\n",
    "\n",
    "#plot the training run\n",
    "plt.plot(epoch_losses)\n",
    "plt.plot(val_losses, 'r')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylim(ymax=30, ymin=0)\n",
    "plt.draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eUo0_OJpxrV4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUo0_OJpxrV4",
    "outputId": "e6dec356-6685-4784-ab4d-fa14f8f21a1e"
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "model.eval()\n",
    "test_dir = \"./data/test_set\"\n",
    "test_rmsd = \"./data/test_rmsd_list.txt\"\n",
    "\n",
    "test_dataset = CGDataset(test_dir, test_rmsd)\n",
    "\n",
    "test_dataloader = dtl.pytorch.GraphDataLoader(val_dataset)\n",
    "\n",
    "test_losses = []\n",
    "for test_graph, test_label in test_dataloader:\n",
    "    test_pred = model(test_graph)\n",
    "    test_loss = F.smooth_l1_loss(test_pred, test_label).item()\n",
    "    test_losses.append(test_loss)\n",
    "    #print(\"Test Prediction: {:.4f}; true RMSD: {:.4f}; loss: {:.4f}\".format(test_pred.item(), test_label.item(), test_loss))\n",
    "\n",
    "test_mean = np.mean(test_losses)\n",
    "test_std = np.std(test_losses)\n",
    "print(\"Mean Test loss: \\t {:.4f}\".format(test_mean))\n",
    "print(\"Std. Dev. of Test loss:  {:.4f}\".format(test_std))\n",
    "print(\"Min loss: \\t \\t {:.4f}\".format(min(test_losses)))\n",
    "print(\"Max Loss: \\t \\t {:.4f}\".format(max(test_losses)))\n",
    "print(\"First Quantile: \\t {:.4f}\".format(np.quantile(test_losses, q=0.25)))\n",
    "print(\"Median: \\t \\t {:.4f}\".format(np.median(test_losses)))\n",
    "print(\"Third Quantile: \\t {:.4f}\".format(np.quantile(test_losses, q=0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a1064-da56-4fab-9ce5-fc0aa0952ef0",
   "metadata": {
    "id": "fb1a1064-da56-4fab-9ce5-fc0aa0952ef0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3d_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
